{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zachary/miniconda3/envs/mel-gen/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 178 prompts from prompt_file.json.\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import torch\n",
    "\n",
    "from utils.audio_segment_utils import segment_audio\n",
    "from utils.riffusion_utils import audio_array_to_image\n",
    "from cnet_riff_preprocessing import create_prompt_file, append_to_prompt_file, generate_and_save_control\n",
    "from utils import spleeter_utils\n",
    "\n",
    "\n",
    "############################################################################################################################################\n",
    "\"\"\"\n",
    "PARAMETERS (edit this section)\n",
    "\"\"\"\n",
    "############################################################################################################################################\n",
    "opt = {}\n",
    "# for control methods, choose any combination from: \"fullspec\", \"canny\", \"thresh\", \"bpf\", \"sobel\", \"sobeldenoise\"\n",
    "opt[\"control_methods\"] = [\"fullspec\", \"canny\", \"sobel\", \"sobeldenoise\"]\n",
    "# where to load data from \n",
    "opt[\"root_data_dir\"] = os.path.join('../','gtzan')\n",
    "opt[\"raw_audio_dir\"] = os.path.join(opt[\"root_data_dir\"],'raw-audio')\n",
    "opt[\"prompt_labels_filepath\"] = os.path.join(opt[\"root_data_dir\"],'prompt_labels.json')\n",
    "\n",
    "# where to save data to\n",
    "opt[\"data_root\"] = os.path.join('gtzan-preprocessed')\n",
    "# true to print information about preprocessing as script runs\n",
    "opt[\"verbose\"] = True\n",
    "# true to wipe anything present in existing prompt files\n",
    "opt[\"new_prompt_files\"] = False\n",
    "\n",
    "\n",
    "# parameters for control generation (if needed)\n",
    "opt[\"fs\"] = 44100\n",
    "opt[\"canny_low_thresh\"] = 150\n",
    "opt[\"canny_high_thresh\"] = 200\n",
    "opt[\"denoise_h\"] = 15\n",
    "\n",
    "# parameters for framing\n",
    "opt[\"frame_overlap\"] = 0 # percentage of frames overlapped, between 0 and 1\n",
    "opt[\"frame_len_seconds\"] = 5.110 # length of frame, in seconds\n",
    "opt[\"frame_min_power_prop\"] = 0.4 # minimum ratio between power in frame and power of stem, between 0 and 1\n",
    "############################################################################################################################################\n",
    "\n",
    "\"\"\"\n",
    "From a dictionary of numpy arrays of the source and generated stems, make the trianing example desired.\n",
    "\"\"\"\n",
    "def make_train_example(source_arr, target_arr, prompt, audio_filename, ex_no, opt):\n",
    "\n",
    "    # path naming \n",
    "    train_example_name = f'{audio_filename}_e{ex_no}.jpg'\n",
    "    target_path = os.path.join(opt[\"target_root\"], train_example_name)\n",
    "\n",
    "    # mix target stems and generate/save spectrogram\n",
    "    target_spectrogram = audio_array_to_image(target_arr, \n",
    "                                  save_img=True,\n",
    "                                  outpath=target_path[:-4],\n",
    "                                  sample_rate=opt[\"fs\"],\n",
    "                                  device=opt[\"device\"],\n",
    "                                  image_extension=\"jpg\")\n",
    "    \n",
    "    # mix source stems and make spectrogram\n",
    "    source_spectrogram = audio_array_to_image(source_arr, \n",
    "                                  save_img=False,\n",
    "                                  outpath=\"\",\n",
    "                                  sample_rate=opt[\"fs\"],\n",
    "                                  device=opt[\"device\"])\n",
    "\n",
    "    for control_method in opt[\"control_methods\"]:\n",
    "        source_path = os.path.join(opt[\"data_root\"], \"source-\"+control_method, train_example_name)\n",
    "        # generate control example for each method desired\n",
    "        generate_and_save_control(source_spectrogram, source_path, control_method, opt)\n",
    "        # add source example to respective prompt file\n",
    "        append_to_prompt_file(opt[\"data_root\"], [source_path], [target_path], prompt, prompt_filename=f\"prompt-{control_method}.json\", verbose=False)\n",
    "\n",
    "    if opt[\"verbose\"]:\n",
    "        print(f\"     {ex_no} - prompt: {prompt}\")\n",
    "    ex_no += 1\n",
    "    return ex_no\n",
    "\n",
    "# tracking\n",
    "num_examples_total = 0\n",
    "time_start = time()\n",
    "\n",
    "# cuda if possible\n",
    "if torch.cuda.is_available():\n",
    "    opt[\"device\"] = \"cuda\"\n",
    "else:\n",
    "    opt[\"device\"] = \"cpu\"\n",
    "\n",
    "# control/target data folders\n",
    "opt[\"control_roots\"] = [os.path.join(opt[\"data_root\"], \"source-\"+mthd) for mthd in opt[\"control_methods\"]]\n",
    "opt[\"target_root\"] = os.path.join(opt[\"data_root\"], 'target')\n",
    "\n",
    "# make all directories needed\n",
    "os.makedirs(opt[\"data_root\"], exist_ok=True)\n",
    "for control_root in opt[\"control_roots\"]:\n",
    "    os.makedirs(control_root, exist_ok=True)\n",
    "os.makedirs(opt[\"target_root\"], exist_ok=True)\n",
    "\n",
    "# get all data examples\n",
    "audio_files = os.listdir(opt[\"raw_audio_dir\"])\n",
    "\n",
    "# get all prompts in prompt_file as dictionary\n",
    "prompt_dict = {}\n",
    "p_count = 0\n",
    "with open(opt[\"prompt_labels_filepath\"], 'r') as prompt_file:\n",
    "    for line in prompt_file:\n",
    "        data = json.loads(line)\n",
    "        prompt_dict[data['file']] = data['prompt']\n",
    "        p_count += 1\n",
    "if opt[\"verbose\"]: print(f\"Read {p_count} prompts from prompt_file.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUDIO FILE 0/178:\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'pretrained_models/2stems', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.7\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object Estimator.predict at 0x17b6510b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zachary/miniconda3/envs/mel-gen/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 650, in predict\n",
      "    yield {\n",
      "  File \"/Users/zachary/miniconda3/envs/mel-gen/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/zachary/miniconda3/envs/mel-gen/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 5874, in get_controller\n",
      "    yield g\n",
      "  File \"/Users/zachary/miniconda3/envs/mel-gen/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/zachary/miniconda3/envs/mel-gen/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 5684, in get_controller\n",
      "    raise AssertionError(\n",
      "AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.ops.Graph'> objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Apply unet for accompaniment_spectrogram\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from pretrained_models/2stems/model\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "num_file = 0\n",
    "audio_file = audio_files[num_file]\n",
    "\n",
    "if opt[\"verbose\"]:\n",
    "    print(f\"AUDIO FILE {num_file}/{len(audio_files)}:\")\n",
    "\n",
    "audio_filename = audio_file[:audio_file.index(\".wav\")]\n",
    "\n",
    "# audio splitting\n",
    "splits = spleeter_utils.separate_audio(os.path.join(opt[\"raw_audio_dir\"], audio_file), fs=opt[\"fs\"], stem_num=2)\n",
    "accompaniment_audio = splits['accompaniment']\n",
    "full_audio = splits['full_audio']\n",
    "vocal_audio = splits['vocals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_audio_segments = segment_audio(full_audio, fs=opt[\"fs\"], num_segments=5, pitch_augment=True)\n",
    "accompaniment_audio_segments = segment_audio(accompaniment_audio, fs=opt[\"fs\"], num_segments=5, pitch_augment=True)\n",
    "vocal_audio_segments = segment_audio(vocal_audio, fs=opt[\"fs\"], num_segments=5, pitch_augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove segments with low vocal power\n",
    "acceptable_inds = []\n",
    "for i, accompaniment_audio_segment in enumerate(accompaniment_audio_segments):\n",
    "        if np.linalg.norm(vocal_audio_segments[i]) > np.linalg.norm(accompaniment_audio_segment)*0.1:\n",
    "            acceptable_inds.append(i)\n",
    "        else:\n",
    "            if opt[\"verbose\"]: print(\"    Vocals not detected in segement \" + str(i))\n",
    "full_audio_segments = full_audio_segments[acceptable_inds]\n",
    "accompaniment_audio_segments = accompaniment_audio_segments[acceptable_inds]\n",
    "vocal_audio_segments = vocal_audio_segments[acceptable_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total number of segments for rock.00011: 35\n"
     ]
    }
   ],
   "source": [
    "if opt[\"verbose\"]:\n",
    "    print(f\"  Total number of segments for {audio_filename}: {full_audio_segments.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EX 0/35:\n",
      "     0 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 1/35:\n",
      "     1 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 2/35:\n",
      "     2 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 3/35:\n",
      "     3 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 4/35:\n",
      "     4 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 5/35:\n",
      "     5 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 6/35:\n",
      "     6 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 7/35:\n",
      "     7 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 8/35:\n",
      "     8 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 9/35:\n",
      "     9 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 10/35:\n",
      "     10 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 11/35:\n",
      "     11 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 12/35:\n",
      "     12 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 13/35:\n",
      "     13 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 14/35:\n",
      "     14 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 15/35:\n",
      "     15 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 16/35:\n",
      "     16 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 17/35:\n",
      "     17 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 18/35:\n",
      "     18 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 19/35:\n",
      "     19 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 20/35:\n",
      "     20 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 21/35:\n",
      "     21 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 22/35:\n",
      "     22 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 23/35:\n",
      "     23 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 24/35:\n",
      "     24 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 25/35:\n",
      "     25 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 26/35:\n",
      "     26 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 27/35:\n",
      "     27 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 28/35:\n",
      "     28 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 29/35:\n",
      "     29 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 30/35:\n",
      "     30 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 31/35:\n",
      "     31 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 32/35:\n",
      "     32 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 33/35:\n",
      "     33 - prompt: Generate an up-tempo female vocal rock melody.\n",
      "  EX 34/35:\n",
      "     34 - prompt: Generate an up-tempo female vocal rock melody.\n"
     ]
    }
   ],
   "source": [
    " # get prompt\n",
    "if audio_file in prompt_dict:\n",
    "    song_prompt = prompt_dict[audio_file]\n",
    "    #10% of the time, also say given background\n",
    "    if np.random.rand() < 0.1:\n",
    "            song_prompt = \"Given background audio, \" + song_prompt\n",
    "else:\n",
    "    song_prompt = \"Generate a vocal melody.\"\n",
    "\n",
    "# make training example for each frame\n",
    "ex_no = 0\n",
    "for i in range(len(full_audio_segments)):\n",
    "    if opt[\"verbose\"]:\n",
    "        print(f\"  EX {i}/{len(full_audio_segments)}:\")\n",
    "    # generate vocal melody from background\n",
    "    ex_no = make_train_example(source_arr=accompaniment_audio_segments[i],\n",
    "                        target_arr=full_audio_segments[i],\n",
    "                        prompt=song_prompt,\n",
    "                        audio_filename=audio_filename,\n",
    "                        ex_no=ex_no,\n",
    "                        opt=opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mel-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
